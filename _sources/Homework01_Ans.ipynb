{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework01_Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings # 忽略警告訊息 \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n",
      "(160, 13) (160,)\n",
      "0.5555555555555556\n",
      "[0, 1, 1, 2, 1, 1, 2, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 1]\n",
      "[0, 1, 0, 2, 1, 1, 0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 2, 1]\n",
      "0.9444444444444444\n",
      "[0, 1, 1, 2, 1, 1, 2, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 1]\n",
      "[0, 1, 0, 2, 1, 1, 0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 2, 1]\n",
      "存取/取用:\n",
      " [0, 1, 1, 2, 1, 1, 2, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 1]\n",
      "[0, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 0, 2, 0, 0, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets     # 引用 Scikit-Learn 中的 套件 datasets\n",
    "\n",
    "ds = datasets.load_wine()\n",
    "print(ds.DESCR)                  # DESCR: description，描述載入內容\n",
    "\n",
    "# 1. Dataset\n",
    "X =pd.DataFrame(ds.data, columns=ds.feature_names)\n",
    "# print(X.head())\n",
    "y = ds.target\n",
    "# print(y)\n",
    "\n",
    "# # 2. Data clean\n",
    "# print(X.isna().sum())\n",
    "\n",
    "# # 3. Date Feturing\n",
    "# # None\n",
    "\n",
    "# 4. Split\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "# 得到 (160, 13) (160,)\n",
    "# print('------------------------------------------')\n",
    "\n",
    "# 5-1. Define and train the KNN model\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "clf = KNN(n_neighbors=3)\n",
    "\n",
    "# 訓練\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 打分數\n",
    "print(clf.score(X_test, y_test))\n",
    "# score = 0.72\n",
    "\n",
    "# 驗證答案\n",
    "print(list(y_test))\n",
    "print(list(clf.predict(X_test)))\n",
    "# [0, 0, 1, 1, 1, 0, 2, 0, 2, 1, 0, 0, 0, 1, 0, 2, 2, 1]\n",
    "# [0, 0, 1, 1, 1, 0, 2, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 2]\n",
    "# 錯 5 個\n",
    "\n",
    "# 5-2. Define and train the LogisticRegression model\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "clf2 = lr(solver='liblinear')\n",
    "# 訓練\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# 打分數\n",
    "print(clf2.score(X_test, y_test))\n",
    "# score = 1.0\n",
    "\n",
    "# 驗證答案\n",
    "print(list(y_test))\n",
    "print(list(clf.predict(X_test)))\n",
    "# [2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 0, 2, 1, 2, 0, 2, 1, 1]\n",
    "# [2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 0, 2, 1, 2, 0, 2, 1, 1]\n",
    "# 全對!\n",
    "\n",
    "X_test.to_csv('wine_test.csv', index=False)\n",
    "\n",
    "# 補充：存取 & 取用模型\n",
    "import joblib as jb\n",
    "# 存取\n",
    "jb.dump(clf2, 'wine.joblib')\n",
    "\n",
    "# 讀取\n",
    "X = pd.read_csv('wine_test.csv')\n",
    "print('存取/取用:\\n', list(y_test))\n",
    "print(list(clf2.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, T-Cells (a type of white blood cells)\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, thyroid stimulating hormone\n",
      "      - s5      ltg, lamotrigine\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
      "          age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
      "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
      "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
      "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
      "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
      "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
      "\n",
      "           s4        s5        s6  \n",
      "0   -0.002592  0.019908 -0.017646  \n",
      "1   -0.039493 -0.068330 -0.092204  \n",
      "2   -0.002592  0.002864 -0.025930  \n",
      "3    0.034309  0.022692 -0.009362  \n",
      "4   -0.002592 -0.031991 -0.046641  \n",
      "..        ...       ...       ...  \n",
      "437 -0.002592  0.031193  0.007207  \n",
      "438  0.034309 -0.018118  0.044485  \n",
      "439 -0.011080 -0.046879  0.015491  \n",
      "440  0.026560  0.044528 -0.025930  \n",
      "441 -0.039493 -0.004220  0.003064  \n",
      "\n",
      "[442 rows x 10 columns]\n",
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n",
      "age    0\n",
      "sex    0\n",
      "bmi    0\n",
      "bp     0\n",
      "s1     0\n",
      "s2     0\n",
      "s3     0\n",
      "s4     0\n",
      "s5     0\n",
      "s6     0\n",
      "dtype: int64\n",
      "(353, 10) (353,)\n",
      "0.52\n",
      "Coefficients:  [  -2.03845894 -194.11551456  550.21170712  304.48207991 -325.82038269\n",
      "  225.720956   -121.35686255  -11.42158995  605.60305459   47.54684467]\n",
      "Intercept:  150.40825681346845\n",
      "Mean squared error: 2984.005812832794\n",
      "Coefficient of determination: 0.5195212553672293\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, neighbors     # 引用 Scikit-Learn 中的 套件 datasets\n",
    "\n",
    "ds = datasets.load_diabetes()\n",
    "print(ds.DESCR)                  # DESCR: description，描述載入內容\n",
    "\n",
    "# 1. Dataset\n",
    "X = pd.DataFrame(ds.data, columns=ds.feature_names)\n",
    "y = ds.target\n",
    "print(X)    # 經過標準化: (X-m)/sigma，平均為 0 / 標準差為 1\n",
    "print(y)\n",
    "\n",
    "# 2. Data clean\n",
    "print(X.isna().sum())\n",
    "\n",
    "# 3. Date Feturing\n",
    "# None\n",
    "\n",
    "# 4. Split\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "# 得到 (353, 10) (353,)\n",
    "\n",
    "# 5. Define and train the LinearRegression model\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "clf = lr()\n",
    "\n",
    "# 訓練\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 打分數\n",
    "print(f'{clf.score(X_test, y_test):.2}')\n",
    "# score = 0.50\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Coefficients (一次項式係數)\n",
    "# y = w1*x1 + w2*x2 + w3*x3 ... w10*x10 + b\n",
    "print('Coefficients: ', clf.coef_)\n",
    "print('Intercept: ', clf.intercept_)\n",
    "\n",
    "# MSE (均方誤差)\n",
    "# 1/n * sum(y_pred-y_test)\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "\n",
    "# Coefficient of determination (判定係數)\n",
    "# 越接近 1 越好\n",
    "print(f'Coefficient of determination: {r2_score(y_test, y_pred)}')\n",
    "\n",
    "# # 驗證答案\n",
    "# print(list(y_test))\n",
    "# print(list(clf.predict(X_test)))\n",
    "\n",
    "# # # 補充：存取 & 取用模型\n",
    "# # import joblib as jb\n",
    "# # # 存取\n",
    "# # jb.dump(clf2, 'wine.joblib')\n",
    "\n",
    "# # # 讀取\n",
    "# # X = pd.read_csv('wine_test.csv')\n",
    "# # print('存取/取用:\\n', list(y_test))\n",
    "# # print(list(clf2.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     total_bill  tip     sex smoker   day    time  size\n",
      "0          17.0 1.01  Female     No   Sun  Dinner     2\n",
      "1          10.3 1.66    Male     No   Sun  Dinner     3\n",
      "2          21.0  3.5    Male     No   Sun  Dinner     3\n",
      "3          23.7 3.31    Male     No   Sun  Dinner     2\n",
      "4          24.6 3.61  Female     No   Sun  Dinner     4\n",
      "..          ...  ...     ...    ...   ...     ...   ...\n",
      "239        29.0 5.92    Male     No   Sat  Dinner     3\n",
      "240        27.2  2.0  Female    Yes   Sat  Dinner     2\n",
      "241        22.7  2.0    Male    Yes   Sat  Dinner     2\n",
      "242        17.8 1.75    Male     No   Sat  Dinner     2\n",
      "243        18.8  3.0  Female     No  Thur  Dinner     2\n",
      "\n",
      "[244 rows x 7 columns]\n",
      "total_bill    0\n",
      "tip           0\n",
      "sex           0\n",
      "smoker        0\n",
      "day           0\n",
      "time          0\n",
      "size          0\n",
      "dtype: int64\n",
      "['Sun' 'Sat' 'Thur' 'Fri']\n",
      "day\n",
      "Fri    2.73\n",
      "Sat    2.99\n",
      "Sun    3.26\n",
      "Thur   2.77\n",
      "Name: tip, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 6) (195,)\n",
      "------------------------------------------\n",
      "score = 0.41\n",
      "[5.0, 1.44, 1.17, 2.0, 2.71, 6.5, 2.0, 2.05, 3.55, 1.5, 3.71, 5.65, 10.0, 2.0, 3.23, 3.92, 2.0, 1.32, 4.0, 3.48, 1.48, 3.0, 2.0, 1.75, 2.75, 3.0, 2.0, 5.16, 3.0, 3.0, 3.5, 2.0, 2.54, 3.76, 4.34, 2.5, 3.23, 2.74, 3.31, 3.5, 3.09, 1.5, 2.5, 3.0, 2.88, 1.0, 2.0, 1.98, 3.5]\n",
      "[6.3, 1.7, 4.0, 4.3, 2.7, 3.5, 3.0, 3.9, 4.1, 2.4, 2.8, 3.1, 5.8, 2.8, 2.6, 3.1, 3.1, 2.0, 3.2, 3.6, 1.9, 3.0, 2.6, 2.8, 3.0, 2.9, 2.3, 3.8, 2.5, 2.7, 2.9, 2.1, 2.7, 3.1, 3.9, 2.4, 2.5, 2.9, 3.3, 2.7, 4.1, 2.1, 2.4, 2.6, 3.0, 1.1, 2.6, 2.0, 2.8]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets     # 引用 Scikit-Learn 中的 套件 datasets\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.3}')           # 轉換 '科學記號' to '浮點數'\n",
    "\n",
    "\n",
    "df = pd.read_csv('tips.csv')\n",
    "print(df)\n",
    "\n",
    "# 1. Dataset\n",
    "X = df.drop('tip', axis=1)\n",
    "# print(X.head())\n",
    "y = df['tip']\n",
    "# print(y.head())\n",
    "\n",
    "# 2. Data clean\n",
    "# 2-1. isna\n",
    "print(df.isna().sum())\n",
    "\n",
    "# 2-2. coding coulumn item\n",
    "print(X['day'].unique())\n",
    "\n",
    "gb = df.groupby(['day'])['tip'].mean()\n",
    "print(gb)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.barplot(gb.index, gb.values)\n",
    "plt.savefig('pic_18.png')\n",
    "plt.show()\n",
    "\n",
    "X['sex'].replace({'Female' : 0, 'Male' : 1}, inplace=True)\n",
    "X['smoker'].replace({'Yes' : 0, 'No' : 1}, inplace=True)\n",
    "X['day'].replace({'Thur' : 0, 'Fri' : 0, 'Sat' : 2, 'Sun' : 3}, inplace=True)\n",
    "X['time'].replace({'Lunch' : 0, 'Dinner' : 1}, inplace=True)\n",
    "# print(X)\n",
    "# print(y)\n",
    "\n",
    "# 3. Date Feturing\n",
    "\n",
    "# 4. Split\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "# test_size=0.2 : 測試用資料為 20%\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "# 得到 (195, 6) (195,)\n",
    "print('------------------------------------------')\n",
    "\n",
    "\n",
    "# 5. Define and train the LinearRegression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f'score = {clf.score(X_test, y_test):.2}')\n",
    "\n",
    "# 驗證答案\n",
    "print(list(y_test))\n",
    "b = [float(f'{i:.2}') for i in clf.predict(X_test)]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
